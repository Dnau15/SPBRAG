{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitrii/micromamba/envs/spbrag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymilvus import MilvusClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import trange\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from typing import Any\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/test.csv')\n",
    "contexts = dataset[\"context\"].dropna().unique().tolist()\n",
    "qa_pairs = dataset.apply(\n",
    "    lambda row: {\n",
    "        \"question\": row[\"question\"],\n",
    "        \"answer\": row[\"answers\"],\n",
    "        \"context\": row[\"context\"],\n",
    "        \"need_retrieval\": row['need_retrieval']\n",
    "    },\n",
    "    axis=1\n",
    ").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"spbrag\"\n",
    "embedding_dim = 768\n",
    "\n",
    "milvus_client = MilvusClient(uri=\"../data/milvus_demo.db\")\n",
    "\n",
    "if milvus_client.has_collection(collection_name):\n",
    "    milvus_client.drop_collection(collection_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "milvus_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    dimension=embedding_dim,\n",
    "    metric_type=\"L2\",\n",
    "    auto_id=True,\n",
    "    primary_field_name=\"id\",\n",
    "    vector_field_name=\"embedding\",\n",
    "    enable_dynamic_field=True,\n",
    "    index_params={\n",
    "        \"index_type\": \"IVF_FLAT\",\n",
    "        \"metric_type\": \"L2\",\n",
    "        \"params\": {\"nlist\": 128},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 39.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 100 documents\n",
      "Inserted 200 documents\n",
      "Inserted 300 documents\n",
      "Inserted 330 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def insert_documents(contexts):\n",
    "    documents = []\n",
    "    for context in contexts:\n",
    "        documents.append(\n",
    "            {\"text\": context, \"embedding\": embedding_model.encode(context).tolist()}\n",
    "        )\n",
    "\n",
    "    # Insert in batches of 100\n",
    "    for i in trange(0, len(documents), 100):\n",
    "        batch = documents[i : i + 100]\n",
    "        milvus_client.insert(collection_name, batch)\n",
    "        print(f\"Inserted {i + len(batch)} documents\")\n",
    "\n",
    "\n",
    "insert_documents(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=100,\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./bert-text-classification-model\"\n",
    "num_labels = 2\n",
    "\n",
    "classificator = BertForSequenceClassification.from_pretrained(\n",
    "    model_path, num_labels=num_labels\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "classificator.to(device)\n",
    "\n",
    "tokenizer_path = \"./bert-text-classification-model\"\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def predict_class(\n",
    "    model,\n",
    "    text: str,\n",
    "    tokenizer: Any,\n",
    "    device: torch.device,\n",
    "    max_length: int = 512,\n",
    ") -> int:\n",
    "    model.eval()\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items() if k != \"idx\"}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What did Yongle want to trade with Tibet?\n",
      "Query: What did Yongle want to trade with Tibet? | Retrieval: Need Retrieval\n",
      "Extract the cinema industry and the percentage box office share occupied by that industry in the format {Industry} - {Percentage} and show as a bullet list. If no percentage specified just list the industry name. Indian cinema is composed of multilingual and multi-ethnic film art. In 2019, Hindi cinema represented 44% of box office revenue, followed by Telugu and Tamil film industries, each representing 13%, Malayalam and Kannada film industries, each representing 5%. Other prominent languages in the Indian film industry include Bengali, Marathi, Odia, Punjabi, Gujarati and Bhojpuri. As of 2022, the combined revenue of South Indian film industries has surpassed that of the Mumbai-based Hindi film industry (Bollywood). As of 2022, Telugu cinema leads Indian cinema's box-office revenue.[details 2]\n",
      "Query: Extract the cinema industry and the percentage box office share occupied by that industry in the format {Industry} - {Percentage} and show as a bullet list. If no percentage specified just list the industry name. Indian cinema is composed of multilingual and multi-ethnic film art. In 2019, Hindi cinema represented 44% of box office revenue, followed by Telugu and Tamil film industries, each representing 13%, Malayalam and Kannada film industries, each representing 5%. Other prominent languages in the Indian film industry include Bengali, Marathi, Odia, Punjabi, Gujarati and Bhojpuri. As of 2022, the combined revenue of South Indian film industries has surpassed that of the Mumbai-based Hindi film industry (Bollywood). As of 2022, Telugu cinema leads Indian cinema's box-office revenue.[details 2] | Retrieval: No Need\n",
      "Adapt the text to make it relevant for a corporate setting, maintaining the core message. Rosie doesn't feel she's ready for the exam, but she can't study anymore. She'll go to bed and rest. \n",
      "Query: Adapt the text to make it relevant for a corporate setting, maintaining the core message. Rosie doesn't feel she's ready for the exam, but she can't study anymore. She'll go to bed and rest.  | Retrieval: No Need\n",
      "What magazine did Beyoncé write a story for about her earlier hiatus?\n",
      "Query: What magazine did Beyoncé write a story for about her earlier hiatus? | Retrieval: Need Retrieval\n",
      "What did the China Digital Times report?\n",
      "Query: What did the China Digital Times report? | Retrieval: Need Retrieval\n",
      "Who are enlightened people who vow to continue being reborn?\n",
      "Query: Who are enlightened people who vow to continue being reborn? | Retrieval: Need Retrieval\n",
      "By May 13, how many troops had been added to the rescue efforts?\n",
      "Query: By May 13, how many troops had been added to the rescue efforts? | Retrieval: Need Retrieval\n",
      "Convert the text into a high school yearbook quote Obama vows to cut contracts by 10 percent The presidential candidates have gotten in a bidding war over promises to trim federal fat, with both using the issue to try to portray themselves as the one to shake up Washington at a time when voters are disgusted with government. Sen. Barack Obama announced Monday that he would cut federal spending on contractors by “at least 10 percent\" — an effort to move in on his rival's signature issue of budget earmarks. Story Continued Below “Barack Obama will reform federal contracting and reduce the number of contractors, saving $40 billion a\n",
      "Query: Convert the text into a high school yearbook quote Obama vows to cut contracts by 10 percent The presidential candidates have gotten in a bidding war over promises to trim federal fat, with both using the issue to try to portray themselves as the one to shake up Washington at a time when voters are disgusted with government. Sen. Barack Obama announced Monday that he would cut federal spending on contractors by “at least 10 percent\" — an effort to move in on his rival's signature issue of budget earmarks. Story Continued Below “Barack Obama will reform federal contracting and reduce the number of contractors, saving $40 billion a | Retrieval: No Need\n",
      "What is Mr. Hinx's job?\n",
      "Query: What is Mr. Hinx's job? | Retrieval: Need Retrieval\n",
      "On what island is Cornell Tech located?\n",
      "Query: On what island is Cornell Tech located? | Retrieval: Need Retrieval\n"
     ]
    }
   ],
   "source": [
    "for qa in qa_pairs[:10]:\n",
    "    query = qa['question']\n",
    "    print(query)\n",
    "    predicted_class = predict_class(classificator, query, tokenizer, device)\n",
    "    r = 'Need Retrieval' if predicted_class else \"No Need\"\n",
    "    print(f\"Query: {query} | Retrieval: {r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(llm: Any, question: str, classificator, tokenizer, device, top_k: int = 3, context_len: int = 400) -> str:\n",
    "    query_embedding = embedding_model.encode(question).tolist()\n",
    "\n",
    "    predicted_class = predict_class(classificator, query, tokenizer, device)\n",
    "    \n",
    "    search_results = milvus_client.search(\n",
    "        collection_name=collection_name,\n",
    "        data=[query_embedding],\n",
    "        limit=top_k,\n",
    "        output_fields=[\"text\"],\n",
    "    )\n",
    "\n",
    "    contexts = (\n",
    "        [hit[\"entity\"][\"text\"] for hit in search_results[0]] if search_results else []\n",
    "    )\n",
    "    context_str = \" \".join(contexts)[:context_len]\n",
    "    template = (\n",
    "        \"Answer the question based on context:\\nContext: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "        if contexts and predicted_class\n",
    "        else \"Answer this question:\\nQuestion: {question}\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    prompt_template = PromptTemplate.from_template(template)\n",
    "    llm_chain = prompt_template | llm\n",
    "\n",
    "    chain_input = {\"question\": question}\n",
    "    if contexts:\n",
    "        chain_input[\"context\"] = context_str\n",
    "\n",
    "    response = llm_chain.invoke(chain_input)\n",
    "    return response, predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing RAG system...\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitrii/micromamba/envs/spbrag/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What did Yongle want to trade with Tibet? | Predicted class: 1 | True class: 1\n",
      "Generated answer:  Tea, horses, and salt.\n",
      "True answer: tea, horses, and salt\n",
      "Question: Extract the cinema industry and the percentag | Predicted class: 0 | True class: 0\n",
      "Generated answer:  - Hindi - 44%\n",
      "- Telugu - 13%\n",
      "- Tamil - 13%\n",
      "- Malayalam - 5%\n",
      "- Kannada - 5%\n",
      "- Bengali\n",
      "- Marathi\n",
      "- Od\n",
      "True answer: nan\n",
      "Question: Adapt the text to make it relevant for a corp | Predicted class: 0 | True class: 0\n",
      "Generated answer:  \"Jane is feeling less prepared for the upcoming board meeting, having exhausted all her preparation\n",
      "True answer: nan\n",
      "Question: What magazine did Beyoncé write a story for a | Predicted class: 1 | True class: 1\n",
      "Generated answer:  Vogue\n",
      "True answer: Essence\n",
      "Question: What did the China Digital Times report? | Predicted class: 1 | True class: 1\n",
      "Generated answer:  The China Digital Times reported that Foxconn, Apple's manufacturer, initially denied labor abuses \n",
      "True answer: a close analysis by an alleged Chinese construction engineer\n",
      "Question: Who are enlightened people who vow to continu | Predicted class: 1 | True class: 1\n",
      "Generated answer:  Bodhisattvas\n",
      "True answer: bodhisattvas\n",
      "Question: By May 13, how many troops had been added to  | Predicted class: 1 | True class: 1\n",
      "Generated answer:  135,000\n",
      "True answer: 15,600\n",
      "Question: Convert the text into a high school yearbook  | Predicted class: 0 | True class: 0\n",
      "Generated answer:  \"Aiming to trim the fat, I vow to reduce federal spending on contracts by at least 10%, saving taxp\n",
      "True answer: nan\n",
      "Question: What is Mr. Hinx's job? | Predicted class: 1 | True class: 1\n",
      "Generated answer:  Mr. Hinx is a Spectre assassin.\n",
      "True answer: assassin\n",
      "Question: On what island is Cornell Tech located? | Predicted class: 1 | True class: 1\n",
      "Generated answer:  Roosevelt Island\n",
      "True answer: Roosevelt Island\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTesting RAG system...\\n\")\n",
    "print(\"-\" * 160)\n",
    "\n",
    "correct = 0\n",
    "num_samples = 10\n",
    "for qa in qa_pairs[:num_samples]:\n",
    "    query = qa[\"question\"]\n",
    "    result, predicted_class = rag_query(\n",
    "        llm, qa[\"question\"], classificator, tokenizer, device\n",
    "    )\n",
    "    generated_answer = result  # [\"answer\"]\n",
    "    true_answer = qa[\"answer\"]\n",
    "\n",
    "    need_retrieval = qa[\"need_retrieval\"]\n",
    "\n",
    "    if need_retrieval == predicted_class:\n",
    "        correct += 1\n",
    "\n",
    "    print(\n",
    "        f\"Question: {qa['question'][:45]} | Predicted class: {predicted_class} | True class: {need_retrieval}\"\n",
    "    )\n",
    "    print(f\"Generated answer: {generated_answer[:100]}\")\n",
    "    print(f\"True answer: {true_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
